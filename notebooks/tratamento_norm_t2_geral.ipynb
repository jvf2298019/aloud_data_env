{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tratamento e Normalização - Conversões T2\n",
        "\n",
        "## Configuração e Importações\n",
        "\n",
        "### Objetivo: Esse pipeline tem por objetivo normalizar os labels de pesquisa que estejam divergentes entre versões de pesquisa criados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Importações concluídas com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Importações básicas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "# Adiciona src ao path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Utilitários de dados\n",
        "from data_utils import (\n",
        "    load_raw_data,\n",
        "    save_processed_data,\n",
        "    remove_duplicates,\n",
        "    handle_missing_values,\n",
        "    detect_outliers,\n",
        "    normalize_column,\n",
        "    process_phone_number,\n",
        "    clean_and_lower_column\n",
        ")\n",
        "\n",
        "# Utilitários SQL\n",
        "from sql_utils import DatabaseConnection as Dbc, load_query_from_file\n",
        "\n",
        "# Utilitários de visualização\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Utilitários de API\n",
        "from api_utils import (\n",
        "    make_request,\n",
        "    get_json,\n",
        "    post_json,\n",
        "    paginated_request,\n",
        "    response_to_dataframe\n",
        ")\n",
        "\n",
        "# Configurações pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Load Database Driver\n",
        "db = Dbc()\n",
        "\n",
        "print('✓ Importações concluídas com sucesso!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Buscar Conversões T2 no banco\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Buscar Conversões T2\n",
        "\n",
        "query = load_query_from_file('select_conversion_t2')\n",
        "df_t2 = db.execute_query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definindo protocolos de Substituição\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_to_replace = {\n",
        "    \"biggest_fluency_challange\": \"biggest_fluency_challenge\",\n",
        "    \"english_proficiency_level\": \"english_level\",\n",
        "    \"monthly_income\": \"monthly_income\",\n",
        "    \"monthly_incomme\": \"monthly_income\",\n",
        "    \"monthly_personal_income\": \"monthly_income\",\n",
        "    \"monthy_income\": \"monthly_income\",\n",
        "    \"has_taken_english_course\": \"took_english_course\",\n",
        "    \"took_english_course_before\": \"took_english_course\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterando por raw_infos e corrigindo Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Antes de iterar, crie as colunas necessárias no DataFrame\n",
        "df_t2['new_conversion_raw_info'] = None\n",
        "df_t2['modified_labels'] = False\n",
        "\n",
        "# Agora, faça a iteração normalmente\n",
        "for idx, row in df_t2.iterrows():\n",
        "    data = row.get(\"conversion_raw_info\")\n",
        "    new_data = {}\n",
        "    modified = False\n",
        "\n",
        "    if isinstance(data, dict):  # Garante que data é um dicionário antes de iterar\n",
        "        for key, value in data.items():\n",
        "            if key in labels_to_replace:\n",
        "                modified = True\n",
        "                new_data[labels_to_replace[key]] = value\n",
        "            else:\n",
        "                new_data[key] = value\n",
        "    else:\n",
        "        # Se data não for um dicionário, apenas atribua o valor como está\n",
        "        new_data = data\n",
        "\n",
        "    df_t2.at[idx, 'new_conversion_raw_info'] = new_data\n",
        "    df_t2.at[idx, 'modified_labels'] = modified\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checking Modifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample de 100 objetos modificados com comparação de labels (mostrando até 3 exemplos):\n",
            "\n",
            "Idx: 80264\n",
            "Chaves antigas: ['form_id', 'token', 'campaign_id', 'score', 'country', 'gender', 'age_range', 'current_occupation', 'biggest_fluency_desire', 'monthy_income', 'english_level', 'took_english_course']\n",
            "Chaves novas : ['form_id', 'token', 'campaign_id', 'score', 'country', 'gender', 'age_range', 'current_occupation', 'biggest_fluency_desire', 'monthly_income', 'english_level', 'took_english_course']\n",
            "Diferenças   : {'idx': 80264, 'antigos_sem_novos': ['monthy_income'], 'novos_sem_antigos': ['monthly_income'], 'valores_atualizados': {}}\n",
            "------------------------------\n",
            "Idx: 57701\n",
            "Chaves antigas: ['score', 'token', 'gender', 'country', 'form_id', 'age_range', 'age_score', 'teb_score', 'reprocessed', 'gender_score', 'has_interest', 'income_score', 'english_level', 'monthy_income', 'occupation_score', 'current_occupation', 'took_english_course', 'biggest_fluency_desire']\n",
            "Chaves novas : ['score', 'token', 'gender', 'country', 'form_id', 'age_range', 'age_score', 'teb_score', 'reprocessed', 'gender_score', 'has_interest', 'income_score', 'english_level', 'monthly_income', 'occupation_score', 'current_occupation', 'took_english_course', 'biggest_fluency_desire']\n",
            "Diferenças   : {'idx': 57701, 'antigos_sem_novos': ['monthy_income'], 'novos_sem_antigos': ['monthly_income'], 'valores_atualizados': {}}\n",
            "------------------------------\n",
            "Idx: 136385\n",
            "Chaves antigas: ['form_id', 'token', 'campaign_id', 'score', 'country', 'gender', 'age_range', 'current_occupation', 'monthy_income', 'biggest_fluency_desire', 'english_level', 'took_english_course']\n",
            "Chaves novas : ['form_id', 'token', 'campaign_id', 'score', 'country', 'gender', 'age_range', 'current_occupation', 'monthly_income', 'biggest_fluency_desire', 'english_level', 'took_english_course']\n",
            "Diferenças   : {'idx': 136385, 'antigos_sem_novos': ['monthy_income'], 'novos_sem_antigos': ['monthly_income'], 'valores_atualizados': {}}\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def sample_comparacao_labels(df, coluna_modificada='modified_labels', coluna_antiga='conversion_raw_info', coluna_nova='new_conversion_raw_info', n_sample=100):\n",
        "    \"\"\"\n",
        "    Seleciona um sample de linhas modificadas e compara os labels alterados.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame com os dados.\n",
        "        coluna_modificada (str): Nome da coluna booleana que indica modificação.\n",
        "        coluna_antiga (str): Nome da coluna com os labels antigos.\n",
        "        coluna_nova (str): Nome da coluna com os labels novos.\n",
        "        n_sample (int): Número de exemplos no sample.\n",
        "\n",
        "    Returns:\n",
        "        diffs (list): Lista de dicionários com as diferenças detectadas.\n",
        "    \"\"\"\n",
        "    modified_true = df[df[coluna_modificada] == True]\n",
        "    amostra = modified_true.sample(min(len(modified_true), n_sample), random_state=42)\n",
        "    diffs = []\n",
        "\n",
        "    for idx, row in amostra.iterrows():\n",
        "        old = row.get(coluna_antiga)\n",
        "        new = row.get(coluna_nova)\n",
        "        if isinstance(old, dict) and isinstance(new, dict):\n",
        "            old_keys = set(old.keys())\n",
        "            new_keys = set(new.keys())\n",
        "            diferencas = {\n",
        "                \"idx\": idx,\n",
        "                \"antigos_sem_novos\": list(old_keys - new_keys),\n",
        "                \"novos_sem_antigos\": list(new_keys - old_keys),\n",
        "                \"valores_atualizados\": {k: {\"antigo\": old[k], \"novo\": new[k]} for k in old_keys & new_keys if old[k] != new[k]}\n",
        "            }\n",
        "            diffs.append({\n",
        "                \"idx\": idx,\n",
        "                \"chaves_antigas\": list(old.keys()),\n",
        "                \"chaves_novas\": list(new.keys()),\n",
        "                \"diferencas\": diferencas,\n",
        "                \"old\": old,\n",
        "                \"new\": new\n",
        "            })\n",
        "        else:\n",
        "            diffs.append({\n",
        "                \"idx\": idx,\n",
        "                \"old\": old,\n",
        "                \"new\": new,\n",
        "                \"diferencas\": \"Um dos valores não é dicionário.\"\n",
        "            })\n",
        "\n",
        "    print(f\"Sample de {len(diffs)} objetos modificados com comparação de labels (mostrando até 3 exemplos):\\n\")\n",
        "    for d in diffs[:3]:  # Mostra só os 3 primeiros exemplos para consulta rápida\n",
        "        print(f\"Idx: {d['idx']}\")\n",
        "        print(\"Chaves antigas:\", d.get(\"chaves_antigas\"))\n",
        "        print(\"Chaves novas :\", d.get(\"chaves_novas\"))\n",
        "        print(\"Diferenças   :\", d[\"diferencas\"])\n",
        "        print(\"-\" * 30)\n",
        "    return diffs\n",
        "\n",
        "# Exemplo de uso:\n",
        "diffs_sample = sample_comparacao_labels(df_t2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtrando por coluna 'new_conversion_raw_info': 195862/195862 registros\n",
            "[12:00:15] Iniciando batch update...\n",
            "Total de registros: 195862\n",
            "Tamanho do lote: 1000\n",
            "[12:04:59] Lote 1/196 - Processados: 1000/195862 (0.5%) - Linhas afetadas neste lote: 1000\n",
            "[12:07:45] Lote 2/196 - Processados: 2000/195862 (1.0%) - Linhas afetadas neste lote: 1000\n",
            "[12:10:15] Lote 3/196 - Processados: 3000/195862 (1.5%) - Linhas afetadas neste lote: 1000\n",
            "[12:12:44] Lote 4/196 - Processados: 4000/195862 (2.0%) - Linhas afetadas neste lote: 1000\n",
            "[12:15:14] Lote 5/196 - Processados: 5000/195862 (2.6%) - Linhas afetadas neste lote: 1000\n",
            "[12:17:43] Lote 6/196 - Processados: 6000/195862 (3.1%) - Linhas afetadas neste lote: 1000\n",
            "[12:20:12] Lote 7/196 - Processados: 7000/195862 (3.6%) - Linhas afetadas neste lote: 1000\n",
            "[12:22:40] Lote 8/196 - Processados: 8000/195862 (4.1%) - Linhas afetadas neste lote: 1000\n",
            "[12:25:12] Lote 9/196 - Processados: 9000/195862 (4.6%) - Linhas afetadas neste lote: 1000\n",
            "[12:27:40] Lote 10/196 - Processados: 10000/195862 (5.1%) - Linhas afetadas neste lote: 1000\n",
            "[12:30:08] Lote 11/196 - Processados: 11000/195862 (5.6%) - Linhas afetadas neste lote: 1000\n",
            "[12:32:37] Lote 12/196 - Processados: 12000/195862 (6.1%) - Linhas afetadas neste lote: 1000\n",
            "[12:35:05] Lote 13/196 - Processados: 13000/195862 (6.6%) - Linhas afetadas neste lote: 1000\n",
            "[12:37:34] Lote 14/196 - Processados: 14000/195862 (7.1%) - Linhas afetadas neste lote: 1000\n",
            "[12:40:03] Lote 15/196 - Processados: 15000/195862 (7.7%) - Linhas afetadas neste lote: 1000\n",
            "[12:42:32] Lote 16/196 - Processados: 16000/195862 (8.2%) - Linhas afetadas neste lote: 1000\n",
            "[12:45:01] Lote 17/196 - Processados: 17000/195862 (8.7%) - Linhas afetadas neste lote: 1000\n",
            "[12:47:29] Lote 18/196 - Processados: 18000/195862 (9.2%) - Linhas afetadas neste lote: 1000\n",
            "[12:49:59] Lote 19/196 - Processados: 19000/195862 (9.7%) - Linhas afetadas neste lote: 1000\n",
            "[12:52:28] Lote 20/196 - Processados: 20000/195862 (10.2%) - Linhas afetadas neste lote: 1000\n",
            "[12:54:58] Lote 21/196 - Processados: 21000/195862 (10.7%) - Linhas afetadas neste lote: 1000\n",
            "[12:57:28] Lote 22/196 - Processados: 22000/195862 (11.2%) - Linhas afetadas neste lote: 1000\n",
            "[12:59:58] Lote 23/196 - Processados: 23000/195862 (11.7%) - Linhas afetadas neste lote: 1000\n",
            "[13:02:27] Lote 24/196 - Processados: 24000/195862 (12.3%) - Linhas afetadas neste lote: 1000\n",
            "[13:04:58] Lote 25/196 - Processados: 25000/195862 (12.8%) - Linhas afetadas neste lote: 1000\n",
            "[13:07:30] Lote 26/196 - Processados: 26000/195862 (13.3%) - Linhas afetadas neste lote: 1000\n",
            "[13:10:00] Lote 27/196 - Processados: 27000/195862 (13.8%) - Linhas afetadas neste lote: 1000\n",
            "[13:12:30] Lote 28/196 - Processados: 28000/195862 (14.3%) - Linhas afetadas neste lote: 1000\n",
            "[13:15:00] Lote 29/196 - Processados: 29000/195862 (14.8%) - Linhas afetadas neste lote: 1000\n",
            "[13:17:31] Lote 30/196 - Processados: 30000/195862 (15.3%) - Linhas afetadas neste lote: 1000\n",
            "[13:20:04] Lote 31/196 - Processados: 31000/195862 (15.8%) - Linhas afetadas neste lote: 1000\n",
            "[13:22:34] Lote 32/196 - Processados: 32000/195862 (16.3%) - Linhas afetadas neste lote: 1000\n",
            "[13:25:04] Lote 33/196 - Processados: 33000/195862 (16.8%) - Linhas afetadas neste lote: 1000\n",
            "[13:27:34] Lote 34/196 - Processados: 34000/195862 (17.4%) - Linhas afetadas neste lote: 1000\n",
            "[13:30:02] Lote 35/196 - Processados: 35000/195862 (17.9%) - Linhas afetadas neste lote: 1000\n",
            "[13:32:30] Lote 36/196 - Processados: 36000/195862 (18.4%) - Linhas afetadas neste lote: 1000\n",
            "[13:34:59] Lote 37/196 - Processados: 37000/195862 (18.9%) - Linhas afetadas neste lote: 1000\n",
            "[13:37:28] Lote 38/196 - Processados: 38000/195862 (19.4%) - Linhas afetadas neste lote: 1000\n",
            "[13:39:57] Lote 39/196 - Processados: 39000/195862 (19.9%) - Linhas afetadas neste lote: 1000\n",
            "[13:42:25] Lote 40/196 - Processados: 40000/195862 (20.4%) - Linhas afetadas neste lote: 1000\n",
            "[13:44:53] Lote 41/196 - Processados: 41000/195862 (20.9%) - Linhas afetadas neste lote: 1000\n",
            "[13:47:21] Lote 42/196 - Processados: 42000/195862 (21.4%) - Linhas afetadas neste lote: 1000\n",
            "[13:50:15] Lote 43/196 - Processados: 43000/195862 (22.0%) - Linhas afetadas neste lote: 1000\n",
            "[13:53:09] Lote 44/196 - Processados: 44000/195862 (22.5%) - Linhas afetadas neste lote: 1000\n",
            "[13:55:43] Lote 45/196 - Processados: 45000/195862 (23.0%) - Linhas afetadas neste lote: 1000\n",
            "[13:58:13] Lote 46/196 - Processados: 46000/195862 (23.5%) - Linhas afetadas neste lote: 1000\n",
            "[14:00:45] Lote 47/196 - Processados: 47000/195862 (24.0%) - Linhas afetadas neste lote: 1000\n",
            "[14:03:12] Lote 48/196 - Processados: 48000/195862 (24.5%) - Linhas afetadas neste lote: 1000\n",
            "[14:05:40] Lote 49/196 - Processados: 49000/195862 (25.0%) - Linhas afetadas neste lote: 1000\n",
            "[14:08:10] Lote 50/196 - Processados: 50000/195862 (25.5%) - Linhas afetadas neste lote: 1000\n",
            "[14:10:40] Lote 51/196 - Processados: 51000/195862 (26.0%) - Linhas afetadas neste lote: 1000\n",
            "[14:13:08] Lote 52/196 - Processados: 52000/195862 (26.5%) - Linhas afetadas neste lote: 1000\n",
            "[14:15:39] Lote 53/196 - Processados: 53000/195862 (27.1%) - Linhas afetadas neste lote: 1000\n",
            "[14:18:10] Lote 54/196 - Processados: 54000/195862 (27.6%) - Linhas afetadas neste lote: 1000\n",
            "[14:20:40] Lote 55/196 - Processados: 55000/195862 (28.1%) - Linhas afetadas neste lote: 1000\n",
            "[14:23:07] Lote 56/196 - Processados: 56000/195862 (28.6%) - Linhas afetadas neste lote: 1000\n",
            "[14:25:36] Lote 57/196 - Processados: 57000/195862 (29.1%) - Linhas afetadas neste lote: 1000\n",
            "[14:28:04] Lote 58/196 - Processados: 58000/195862 (29.6%) - Linhas afetadas neste lote: 1000\n",
            "[14:30:38] Lote 59/196 - Processados: 59000/195862 (30.1%) - Linhas afetadas neste lote: 1000\n",
            "[14:33:07] Lote 60/196 - Processados: 60000/195862 (30.6%) - Linhas afetadas neste lote: 1000\n",
            "[14:35:34] Lote 61/196 - Processados: 61000/195862 (31.1%) - Linhas afetadas neste lote: 1000\n",
            "[14:38:03] Lote 62/196 - Processados: 62000/195862 (31.7%) - Linhas afetadas neste lote: 1000\n",
            "[14:40:31] Lote 63/196 - Processados: 63000/195862 (32.2%) - Linhas afetadas neste lote: 1000\n",
            "[14:42:59] Lote 64/196 - Processados: 64000/195862 (32.7%) - Linhas afetadas neste lote: 1000\n",
            "[14:45:29] Lote 65/196 - Processados: 65000/195862 (33.2%) - Linhas afetadas neste lote: 1000\n",
            "[14:47:58] Lote 66/196 - Processados: 66000/195862 (33.7%) - Linhas afetadas neste lote: 1000\n",
            "[14:50:26] Lote 67/196 - Processados: 67000/195862 (34.2%) - Linhas afetadas neste lote: 1000\n",
            "[14:52:55] Lote 68/196 - Processados: 68000/195862 (34.7%) - Linhas afetadas neste lote: 1000\n",
            "[14:55:26] Lote 69/196 - Processados: 69000/195862 (35.2%) - Linhas afetadas neste lote: 1000\n",
            "[14:57:54] Lote 70/196 - Processados: 70000/195862 (35.7%) - Linhas afetadas neste lote: 1000\n",
            "[15:00:22] Lote 71/196 - Processados: 71000/195862 (36.3%) - Linhas afetadas neste lote: 1000\n",
            "[15:02:52] Lote 72/196 - Processados: 72000/195862 (36.8%) - Linhas afetadas neste lote: 1000\n",
            "[15:05:27] Lote 73/196 - Processados: 73000/195862 (37.3%) - Linhas afetadas neste lote: 1000\n",
            "[15:07:56] Lote 74/196 - Processados: 74000/195862 (37.8%) - Linhas afetadas neste lote: 1000\n",
            "[15:10:24] Lote 75/196 - Processados: 75000/195862 (38.3%) - Linhas afetadas neste lote: 1000\n",
            "[15:12:53] Lote 76/196 - Processados: 76000/195862 (38.8%) - Linhas afetadas neste lote: 1000\n",
            "[15:15:22] Lote 77/196 - Processados: 77000/195862 (39.3%) - Linhas afetadas neste lote: 1000\n",
            "[15:17:51] Lote 78/196 - Processados: 78000/195862 (39.8%) - Linhas afetadas neste lote: 1000\n",
            "[15:20:20] Lote 79/196 - Processados: 79000/195862 (40.3%) - Linhas afetadas neste lote: 1000\n",
            "[15:22:50] Lote 80/196 - Processados: 80000/195862 (40.8%) - Linhas afetadas neste lote: 1000\n",
            "[15:25:18] Lote 81/196 - Processados: 81000/195862 (41.4%) - Linhas afetadas neste lote: 1000\n",
            "[15:27:48] Lote 82/196 - Processados: 82000/195862 (41.9%) - Linhas afetadas neste lote: 1000\n",
            "[15:30:16] Lote 83/196 - Processados: 83000/195862 (42.4%) - Linhas afetadas neste lote: 1000\n",
            "[15:32:46] Lote 84/196 - Processados: 84000/195862 (42.9%) - Linhas afetadas neste lote: 1000\n",
            "[15:35:14] Lote 85/196 - Processados: 85000/195862 (43.4%) - Linhas afetadas neste lote: 1000\n",
            "[15:37:42] Lote 86/196 - Processados: 86000/195862 (43.9%) - Linhas afetadas neste lote: 1000\n",
            "[15:40:10] Lote 87/196 - Processados: 87000/195862 (44.4%) - Linhas afetadas neste lote: 1000\n",
            "[15:42:38] Lote 88/196 - Processados: 88000/195862 (44.9%) - Linhas afetadas neste lote: 1000\n",
            "[15:45:06] Lote 89/196 - Processados: 89000/195862 (45.4%) - Linhas afetadas neste lote: 1000\n",
            "[15:47:34] Lote 90/196 - Processados: 90000/195862 (46.0%) - Linhas afetadas neste lote: 1000\n",
            "[15:50:03] Lote 91/196 - Processados: 91000/195862 (46.5%) - Linhas afetadas neste lote: 1000\n",
            "[15:52:31] Lote 92/196 - Processados: 92000/195862 (47.0%) - Linhas afetadas neste lote: 1000\n",
            "[15:55:00] Lote 93/196 - Processados: 93000/195862 (47.5%) - Linhas afetadas neste lote: 1000\n",
            "[15:57:28] Lote 94/196 - Processados: 94000/195862 (48.0%) - Linhas afetadas neste lote: 1000\n",
            "[15:59:59] Lote 95/196 - Processados: 95000/195862 (48.5%) - Linhas afetadas neste lote: 1000\n",
            "[16:02:27] Lote 96/196 - Processados: 96000/195862 (49.0%) - Linhas afetadas neste lote: 1000\n",
            "[16:04:56] Lote 97/196 - Processados: 97000/195862 (49.5%) - Linhas afetadas neste lote: 1000\n",
            "[16:07:25] Lote 98/196 - Processados: 98000/195862 (50.0%) - Linhas afetadas neste lote: 1000\n",
            "[16:09:53] Lote 99/196 - Processados: 99000/195862 (50.5%) - Linhas afetadas neste lote: 1000\n",
            "[16:12:21] Lote 100/196 - Processados: 100000/195862 (51.1%) - Linhas afetadas neste lote: 1000\n",
            "[16:14:49] Lote 101/196 - Processados: 101000/195862 (51.6%) - Linhas afetadas neste lote: 1000\n",
            "[16:17:19] Lote 102/196 - Processados: 102000/195862 (52.1%) - Linhas afetadas neste lote: 1000\n",
            "[16:19:47] Lote 103/196 - Processados: 103000/195862 (52.6%) - Linhas afetadas neste lote: 1000\n",
            "[16:22:16] Lote 104/196 - Processados: 104000/195862 (53.1%) - Linhas afetadas neste lote: 1000\n",
            "[16:24:43] Lote 105/196 - Processados: 105000/195862 (53.6%) - Linhas afetadas neste lote: 1000\n",
            "[16:27:11] Lote 106/196 - Processados: 106000/195862 (54.1%) - Linhas afetadas neste lote: 1000\n",
            "[16:29:41] Lote 107/196 - Processados: 107000/195862 (54.6%) - Linhas afetadas neste lote: 1000\n",
            "[16:32:09] Lote 108/196 - Processados: 108000/195862 (55.1%) - Linhas afetadas neste lote: 1000\n",
            "[16:34:38] Lote 109/196 - Processados: 109000/195862 (55.7%) - Linhas afetadas neste lote: 1000\n",
            "[16:37:07] Lote 110/196 - Processados: 110000/195862 (56.2%) - Linhas afetadas neste lote: 1000\n",
            "[16:39:37] Lote 111/196 - Processados: 111000/195862 (56.7%) - Linhas afetadas neste lote: 1000\n",
            "[16:42:05] Lote 112/196 - Processados: 112000/195862 (57.2%) - Linhas afetadas neste lote: 1000\n",
            "[16:44:34] Lote 113/196 - Processados: 113000/195862 (57.7%) - Linhas afetadas neste lote: 1000\n",
            "[16:47:03] Lote 114/196 - Processados: 114000/195862 (58.2%) - Linhas afetadas neste lote: 1000\n",
            "[16:49:31] Lote 115/196 - Processados: 115000/195862 (58.7%) - Linhas afetadas neste lote: 1000\n",
            "[16:52:01] Lote 116/196 - Processados: 116000/195862 (59.2%) - Linhas afetadas neste lote: 1000\n",
            "[16:54:30] Lote 117/196 - Processados: 117000/195862 (59.7%) - Linhas afetadas neste lote: 1000\n",
            "[16:56:58] Lote 118/196 - Processados: 118000/195862 (60.2%) - Linhas afetadas neste lote: 1000\n",
            "[16:59:26] Lote 119/196 - Processados: 119000/195862 (60.8%) - Linhas afetadas neste lote: 1000\n",
            "[17:01:54] Lote 120/196 - Processados: 120000/195862 (61.3%) - Linhas afetadas neste lote: 1000\n",
            "[17:04:23] Lote 121/196 - Processados: 121000/195862 (61.8%) - Linhas afetadas neste lote: 1000\n",
            "[17:06:52] Lote 122/196 - Processados: 122000/195862 (62.3%) - Linhas afetadas neste lote: 1000\n",
            "[17:09:20] Lote 123/196 - Processados: 123000/195862 (62.8%) - Linhas afetadas neste lote: 1000\n",
            "[17:11:48] Lote 124/196 - Processados: 124000/195862 (63.3%) - Linhas afetadas neste lote: 1000\n",
            "[17:14:16] Lote 125/196 - Processados: 125000/195862 (63.8%) - Linhas afetadas neste lote: 1000\n",
            "[17:16:44] Lote 126/196 - Processados: 126000/195862 (64.3%) - Linhas afetadas neste lote: 1000\n",
            "[17:19:15] Lote 127/196 - Processados: 127000/195862 (64.8%) - Linhas afetadas neste lote: 1000\n",
            "[17:21:52] Lote 128/196 - Processados: 128000/195862 (65.4%) - Linhas afetadas neste lote: 1000\n",
            "[17:24:21] Lote 129/196 - Processados: 129000/195862 (65.9%) - Linhas afetadas neste lote: 1000\n",
            "[17:26:49] Lote 130/196 - Processados: 130000/195862 (66.4%) - Linhas afetadas neste lote: 1000\n",
            "[17:29:18] Lote 131/196 - Processados: 131000/195862 (66.9%) - Linhas afetadas neste lote: 1000\n",
            "[17:31:46] Lote 132/196 - Processados: 132000/195862 (67.4%) - Linhas afetadas neste lote: 1000\n",
            "[17:34:13] Lote 133/196 - Processados: 133000/195862 (67.9%) - Linhas afetadas neste lote: 1000\n",
            "[17:36:41] Lote 134/196 - Processados: 134000/195862 (68.4%) - Linhas afetadas neste lote: 1000\n",
            "[17:39:09] Lote 135/196 - Processados: 135000/195862 (68.9%) - Linhas afetadas neste lote: 1000\n",
            "[17:41:37] Lote 136/196 - Processados: 136000/195862 (69.4%) - Linhas afetadas neste lote: 1000\n",
            "[17:44:06] Lote 137/196 - Processados: 137000/195862 (69.9%) - Linhas afetadas neste lote: 1000\n",
            "[17:46:33] Lote 138/196 - Processados: 138000/195862 (70.5%) - Linhas afetadas neste lote: 1000\n"
          ]
        }
      ],
      "source": [
        "# ✅ NOVO: 10.000 registros = ~5 segundos (36x mais rápido!)\n",
        "update_query = load_query_from_file('update_conversion_t2_raw_info')\n",
        "\n",
        "resultado = db.update_from_dataframe(\n",
        "    df=df_t2,\n",
        "    query=update_query,\n",
        "    column_mapping={\n",
        "        \"id\": \"conversion_id\",\n",
        "        \"new_conversion_raw_info\": \"new_conversion_raw_info\"\n",
        "    },\n",
        "    filter_column=\"new_conversion_raw_info\",  # Filtra apenas não-nulos\n",
        "    batch_size=1000,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(f\"✅ Sucesso: {resultado['success']}\")\n",
        "print(f\"❌ Falhas: {resultado['failed']}\")\n",
        "print(f\"⏱️ Tempo: {resultado['elapsed_time']:.2f}s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
